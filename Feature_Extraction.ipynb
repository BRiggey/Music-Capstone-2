{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual Libraries USED FROM THE OTHER NOTEBOOK WE ADD MORE OR DELETE WHEN NECESSESARRY \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import distutils\n",
    "import ast\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Librosa (the mother of audio files)\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing TFRecord files: 100%|██████████| 1444/1444 [02:07<00:00, 11.35it/s]\n",
      "Processing TFRecord files: 100%|██████████| 1444/1444 [02:50<00:00,  8.45it/s]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Define the context features\n",
    "context_features = {\n",
    "    'video_id': tf.io.FixedLenFeature([], tf.string),\n",
    "    'start_time_seconds': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'end_time_seconds': tf.io.FixedLenFeature([], tf.float32),\n",
    "    'labels': tf.io.VarLenFeature(tf.int64)\n",
    "}\n",
    "\n",
    "# Define the sequence features\n",
    "sequence_features = {\n",
    "    'audio_embedding': tf.io.VarLenFeature(tf.string)\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "    # Parse the example into context and sequence features\n",
    "    context_data, sequence_data = tf.io.parse_single_sequence_example(\n",
    "        example_proto,\n",
    "        context_features=context_features,\n",
    "        sequence_features=sequence_features\n",
    "    )\n",
    "    \n",
    "    # Decode the audio embeddings from the byte strings\n",
    "    audio_embeddings = tf.io.decode_raw(sequence_data['audio_embedding'].values, tf.uint8)\n",
    "    audio_embeddings = tf.reshape(audio_embeddings, [-1, 128])  # Reshape to [time_steps, embedding_size]\n",
    "    \n",
    "    # Normalize the embeddings to the range [0, 1]\n",
    "    audio_embeddings = tf.cast(audio_embeddings, tf.float32) / 255.0\n",
    "    \n",
    "    # Flatten the embedding to a 1D array\n",
    "    flattened_embedding = tf.reshape(audio_embeddings, [-1])\n",
    "    \n",
    "    # Convert sparse labels to dense\n",
    "    labels = tf.sparse.to_dense(context_data['labels'])\n",
    "    \n",
    "    return flattened_embedding, labels\n",
    "\n",
    "def parse_tfrecords_to_dataframe(tfrecord_files):\n",
    "    # Initialize an empty list to store embeddings and labels\n",
    "    embeddings_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    # Iterate through each TFRecord file in the folder\n",
    "    for tfrecord_file in tqdm(tfrecord_files, desc=\"Processing TFRecord files\"):\n",
    "        raw_dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
    "        parsed_dataset = raw_dataset.map(_parse_function)\n",
    "\n",
    "        # Iterate over the parsed dataset and collect embeddings and labels\n",
    "        for audio_embedding, labels in parsed_dataset:\n",
    "            # Convert to numpy arrays\n",
    "            audio_embedding_np = audio_embedding.numpy()\n",
    "            labels_np = labels.numpy()\n",
    "\n",
    "            # Flatten the label array if necessary\n",
    "            labels_np = labels_np.flatten() if len(labels_np.shape) > 0 else labels_np\n",
    "\n",
    "            # Store the embedding and label\n",
    "            embeddings_list.append(audio_embedding_np)\n",
    "            labels_list.append(labels_np)\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame({\n",
    "        'embedding': embeddings_list,\n",
    "        'labels': labels_list\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Specify the folder containing TFRecord files\n",
    "bal_tfrecord_folder = '.github\\Data\\\\bal_train'\n",
    "bal_tfrecord_files = [os.path.join(bal_tfrecord_folder, f) for f in os.listdir(bal_tfrecord_folder) if f.endswith('.tfrecord')]\n",
    "\n",
    "eval_tfrecord_folder = '.github\\Data\\\\eval'\n",
    "eval_tfrecord_files = [os.path.join(eval_tfrecord_folder, f) for f in os.listdir(eval_tfrecord_folder) if f.endswith('.tfrecord')]\n",
    "eval_df = parse_tfrecords_to_dataframe(eval_tfrecord_files)\n",
    "# Parse the TFRecords into a DataFrame\n",
    "bal_df = parse_tfrecords_to_dataframe(bal_tfrecord_files)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure the 'data' directory exists\n",
    "output_folder = r'.github\\Data'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Paths for saving the CSV files\n",
    "eval_csv_path = os.path.join(output_folder, 'eval_df.csv')\n",
    "bal_csv_path = os.path.join(output_folder, 'bal_df.csv')\n",
    "\n",
    "# Save the DataFrames to CSV\n",
    "eval_df.to_csv(eval_csv_path, index=False)\n",
    "bal_df.to_csv(bal_csv_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape them  embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Embeddings Shape: (7971, 100, 128)\n",
      "Reshaped Embeddings Shape: (7330, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters (adjust as necessary)\n",
    "embedding_length = 128  # This should match the original dimension of each heatmap slice\n",
    "sequence_length = 100  # Adjust to match the desired number of time steps per sequence\n",
    "\n",
    "# Convert the embedding column to a numpy array and reshape each embedding\n",
    "def reshape_embeddings(df, embedding_length, sequence_length):\n",
    "    reshaped_embeddings = []\n",
    "    for embedding in df['embedding']:\n",
    "        # Ensure embedding is a numpy array\n",
    "        embedding = np.array(embedding)\n",
    "        \n",
    "        # Reshape the embedding into (sequence_length, embedding_length)\n",
    "        # If necessary, truncate or pad to the sequence_length\n",
    "        if len(embedding) < sequence_length * embedding_length:\n",
    "            # Pad with zeros if too short\n",
    "            padding_length = sequence_length * embedding_length - len(embedding)\n",
    "            embedding = np.pad(embedding, (0, padding_length), mode='constant')\n",
    "        elif len(embedding) > sequence_length * embedding_length:\n",
    "            # Truncate if too long\n",
    "            embedding = embedding[:sequence_length * embedding_length]\n",
    "        \n",
    "        # Reshape to (sequence_length, embedding_length)\n",
    "        embedding = embedding.reshape((sequence_length, embedding_length))\n",
    "        reshaped_embeddings.append(embedding)\n",
    "\n",
    "    # Convert to numpy array for model input\n",
    "    reshaped_embeddings = np.array(reshaped_embeddings)\n",
    "    return reshaped_embeddings\n",
    "\n",
    "# Apply reshaping to embeddings\n",
    "reshaped_bal_embeddings = reshape_embeddings(bal_df, embedding_length, sequence_length)\n",
    "#reshaped_eval_embeddings = reshape_embeddings(eval_df, embedding_length, sequence_length)\n",
    "# Output the reshaped array's shape to verify\n",
    "print(\"Reshaped Embeddings Shape:\", reshaped_bal_embeddings.shape)\n",
    "#print(\"Reshaped Embeddings Shape:\", reshaped_eval_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for abs(): 'ellipsis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chroma\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Apply chroma extraction to the balanced dataset (assuming reshaped_bal_embeddings is a list of embeddings)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m chroma_features_bal \u001b[38;5;241m=\u001b[39m [extract_chroma(embedding) \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m reshaped_bal_embeddings]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Check the shape of the first chroma feature\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(chroma_features_bal[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[30], line 24\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chroma\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Apply chroma extraction to the balanced dataset (assuming reshaped_bal_embeddings is a list of embeddings)\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m chroma_features_bal \u001b[38;5;241m=\u001b[39m [extract_chroma(embedding) \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m reshaped_bal_embeddings]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Check the shape of the first chroma feature\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(chroma_features_bal[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[30], line 10\u001b[0m, in \u001b[0;36mextract_chroma\u001b[1;34m(embedding)\u001b[0m\n\u001b[0;32m      7\u001b[0m embedding \u001b[38;5;241m=\u001b[39m embedding\u001b[38;5;241m.\u001b[39mflatten()  \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Normalize the embedding to the range [-1, 1] (since librosa expects audio data in this range)\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m embedding \u001b[38;5;241m=\u001b[39m embedding \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mabs(embedding))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Ensure the embedding is of the correct floating-point type\u001b[39;00m\n\u001b[0;32m     13\u001b[0m embedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32(embedding)\n",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for abs(): 'ellipsis'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "# Process a single embedding to extract chroma\n",
    "def extract_chroma(embedding):\n",
    "    # Flatten the (100, 128) embedding to 1D\n",
    "    embedding = embedding.flatten()  \n",
    "\n",
    "    # Normalize the embedding to the range [-1, 1] (since librosa expects audio data in this range)\n",
    "    embedding = embedding / np.max(np.abs(embedding))\n",
    "\n",
    "    # Ensure the embedding is of the correct floating-point type\n",
    "    embedding = np.float32(embedding)\n",
    "\n",
    "    # Set the sampling rate\n",
    "    sr = 22050  # Adjust as needed, typically for audio\n",
    "\n",
    "    # Extract chroma\n",
    "    chroma = librosa.feature.chroma_cqt(y=embedding, sr=sr)\n",
    "\n",
    "    return chroma\n",
    "\n",
    "# Apply chroma extraction to the balanced dataset (assuming reshaped_bal_embeddings is a list of embeddings)\n",
    "chroma_features_bal = [extract_chroma(embedding) for embedding in reshaped_bal_embeddings]\n",
    "\n",
    "# Check the shape of the first chroma feature\n",
    "print(chroma_features_bal[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chroma_features_bal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chroma_features_bal\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chroma_features_bal' is not defined"
     ]
    }
   ],
   "source": [
    "chroma_features_bal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data and training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(reshaped_embeddings, df['embedding '], test_size=0.3, random_state=42)\n",
    " X_test, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Prepare lengths for CTC loss\n",
    "input_lengths_train = np.full((len(X_train),), sequence_length)\n",
    "label_lengths_train = np.array([len(emebedding ) for embedding  in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    [X_train, y_train, input_lengths_train, label_lengths_train],\n",
    "    np.zeros(len(y_train)),  # Dummy target required by Keras for Lambda layer output\n",
    "    validation_data=(\n",
    "        [X_val, y_val, np.full((len(X_val),), sequence_length), np.array([len(labels) for labels in y_val])],\n",
    "        np.zeros(len(y_val))\n",
    "    ),\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    callbacks=[tf.keras.callbacks.ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss')]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
